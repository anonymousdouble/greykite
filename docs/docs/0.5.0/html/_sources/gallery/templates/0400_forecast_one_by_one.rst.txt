
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/templates/0400_forecast_one_by_one.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gallery_templates_0400_forecast_one_by_one.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_templates_0400_forecast_one_by_one.py:


Forecast One By One
===================

A useful feature for short-term forecast in Silverkite model family is autoregression.
Silverkite has an "auto" option for autoregression,
which automatically selects the autoregression lag orders based on the data frequency and forecast horizons.
One important rule of this "auto" option is that the minimum order of autoregression terms
is at least the forecast horizon.
For example, if the forecast horizon is 3 on a daily model,
the minimum order of autoregression is set to 3.
The "auto" option won't have an order of 2 in this case,
because the 3rd day forecast will need the 1st day's observation,
which isn't available at the current time.
Although the model can make predictions with an autoregression lag order less than the forecast horizon
via simulations, it takes longer time to run and is not the preferred behavior in the "auto" option.

However, in many cases, using smaller autoregression lag orders can give more accurate forecast results.
We observe that the only barrier of using an autoregression term of order 2 in the 3-day forecast model
is the 3rd day, while we can use it freely for the first 2 days.
Similarly, we are able to use an autoregression term of order 1 for the 1st day.
In a 3 day forecast, if the accuracy of all 3 days are important, then replacing the first 2 days' models
with shorter autoregression lag orders can improve the accuracy.
The forecast-one-by-one algorithm is designed in this context.

The observations above together bring the idea of the forecast-one-by-one algorithm.
The algorithm allows fitting multiple models with the "auto" option in autoregression,
when one is forecasting with a forecast horizon longer than 1.
For each model, the "auto" option for autoregression selects the smallest
available autoregression lag order and predicts for the corresponding forecast steps,
thus improving the forecast accuracy for the early steps.

In this example, we will cover how to activate the forecast-one-by-one approach
via the ``ForecastConfig`` and the ``Forecaster`` classes.
For a detailed API reference, please see the
`~greykite.framework.templates.autogen.forecast_config.ForecastConfig` and
`~greykite.sklearn.estimator.one_by_one_estimator.OneByOneEstimator` classes.

.. GENERATED FROM PYTHON SOURCE LINES 39-56

.. code-block:: default
   :lineno-start: 40


    import warnings

    warnings.filterwarnings("ignore")

    import plotly
    from greykite.common.data_loader import DataLoader
    from greykite.framework.templates.autogen.forecast_config import ForecastConfig
    from greykite.framework.templates.autogen.forecast_config import ModelComponentsParam
    from greykite.framework.templates.forecaster import Forecaster
    from greykite.framework.templates.model_templates import ModelTemplateEnum
    from greykite.framework.utils.result_summary import summarize_grid_search_results

    # Loads dataset into pandas DataFrame
    dl = DataLoader()
    df = dl.load_peyton_manning()








.. GENERATED FROM PYTHON SOURCE LINES 57-62

The forecast-one-by-one option
------------------------------

The forecast-one-by-one option is specified through the ``forecast_one_by_one`` parameter
in ``ForecastConfig``.

.. GENERATED FROM PYTHON SOURCE LINES 62-72

.. code-block:: default
   :lineno-start: 63


    config = ForecastConfig(
        model_template=ModelTemplateEnum.SILVERKITE.name,
        forecast_horizon=3,
        model_components_param=ModelComponentsParam(
            autoregression=dict(autoreg_dict="auto")
        ),
        forecast_one_by_one=True
    )








.. GENERATED FROM PYTHON SOURCE LINES 73-105

The ``forecast_one_by_one`` parameter can be specified in the following ways

  - **``True``**: every forecast step will be a separate model.
    The number of models equals the forecast horizon.
    In this example, 3 models will be fit with the 3 forecast steps.
  - **``False``**: the forecast-one-by-one method is turned off.
    This is the default behavior and a single model is used for all forecast steps.
  - **A list of integers**: each integer corresponds to a model,
    and it is the number of steps. For example, in a 7 day forecast,
    specifying ``forecast_one_by_one=[1, 2, 4]`` will result in 3 models.
    The first model forecasts the 1st day with forecast horizon 1;
    The second model forecasts the 2nd - 3rd days with forecast horizon 3;
    The third model forecasts the 4th - 7th days with forecast horizon 7.
    In this case, the sum of the list entries must equal the forecast horizon.
  - **an integer ``n``**: every model will account for n steps. The last model
    will account for the rest <n steps. For example in a 7 day forecast,
    specifying ``forecast_one_by_one=2`` will result in 4 models,
    which is equivalent to ``forecast_one_by_one=[2, 2, 2, 1]``.

.. note::
  ``forecast_one_by_one`` is activated only when there are parameters in
  the model that depend on the forecast horizon. Currently the only parameter
  that depends on forecast horizon is ``autoreg_dict="auto"``. If you do not specify
  ``autoreg_dict="auto"``, the ``forecast_one_by_one`` parameter will be ignored.

.. note::
  Forecast-one-by-one fits multiple models to increase accuracy,
  which may cause the training time to increase linearly with the number of models.
  Please make sure your ``forecast_one_by_one`` parameter and forecast horizon
  result in a reasonable number of models.

Next, let's run the model and look at the result.

.. GENERATED FROM PYTHON SOURCE LINES 105-113

.. code-block:: default
   :lineno-start: 106


    # Runs the forecast
    forecaster = Forecaster()
    result = forecaster.run_forecast_config(
        df=df.iloc[-365:].reset_index(drop=True),  # Uses less data to speed up this example.
        config=config
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Fitting 3 folds for each of 1 candidates, totalling 3 fits




.. GENERATED FROM PYTHON SOURCE LINES 114-125

You may see a few warnings like "The future x length is 0,
which doesn't match the model forecast horizon 3,
using only the model with the longest forecast horizon for prediction."
This is an expected behavior when calculating the training errors.
Because the models are mapped to the forecast period only,
but not to the training period. Therefore, only the last model is used to
get the fitted values on the training period.
You don't need to worry about it.

Everything on the ``forecast_result`` level is the same as not activating forecast-one-by-one.
For example, we can view the cross-validation results in the same way.

.. GENERATED FROM PYTHON SOURCE LINES 125-137

.. code-block:: default
   :lineno-start: 126


    # Summarizes the CV results
    cv_results = summarize_grid_search_results(
        grid_search=result.grid_search,
        decimals=1,
        # The below saves space in the printed output. Remove to show all available metrics and columns.
        cv_report_metrics=None,
        column_order=["rank", "mean_test", "split_test", "mean_train", "split_train", "mean_fit_time", "mean_score_time", "params"])
    cv_results["params"] = cv_results["params"].astype(str)
    cv_results.set_index("params", drop=True, inplace=True)
    cv_results.transpose()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th>params</th>
          <th>[]</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>rank_test_MAPE</th>
          <td>1</td>
        </tr>
        <tr>
          <th>mean_test_MAPE</th>
          <td>5.9</td>
        </tr>
        <tr>
          <th>split_test_MAPE</th>
          <td>(11.3, 2.1, 4.2)</td>
        </tr>
        <tr>
          <th>mean_train_MAPE</th>
          <td>3.2</td>
        </tr>
        <tr>
          <th>split_train_MAPE</th>
          <td>(2.8, 3.4, 3.4)</td>
        </tr>
        <tr>
          <th>mean_fit_time</th>
          <td>8.4</td>
        </tr>
        <tr>
          <th>mean_score_time</th>
          <td>2.2</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 138-142

When you need to access estimator level attributes, for example, model summary or component plots,
the returned result will be a list of the original type, because we fit multiple models.
The model summary list can be accessed in the same way and you can use index to get the model summary
for a single model.

.. GENERATED FROM PYTHON SOURCE LINES 142-149

.. code-block:: default
   :lineno-start: 143


    # Gets the model summary list
    one_by_one_estimator = result.model[-1]
    summaries = one_by_one_estimator.summary()
    # Prints the model summary for 1st model only
    print(summaries[0])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    ================================ Model Summary =================================

    Number of observations: 367,   Number of features: 98
    Method: Ridge regression
    Number of nonzero features: 98
    Regularization parameter: 0.3039

    Residuals:
             Min           1Q       Median           3Q          Max
         -0.8742      -0.1628     -0.04904      0.09108        3.086

                Pred_col  Estimate Std. Err Pr(>)_boot sig. code                95%CI
               Intercept     6.302    0.161     <2e-16       ***         (5.982, 6.6)
     events_C...New Year    0.3751   0.2887      0.172              (-0.1164, 0.8291)
     events_C...w Year-1   -0.1972   0.2131      0.342              (-0.5858, 0.2344)
     events_C...w Year-2   -0.1852    0.164      0.218                 (-0.4766, 0.2)
     events_C...w Year+1  -0.09128   0.2074      0.610              (-0.3493, 0.4496)
     events_C...w Year+2   -0.1268   0.2202      0.564              (-0.4856, 0.3978)
    events_Christmas Day   -0.4262   0.2485      0.020         *         (-0.707, 0.)
     events_C...as Day-1    -0.545    0.293      0.004        **        (-0.7914, 0.)
     events_C...as Day-2   -0.1055   0.0932      0.184            (-0.2788, 0.004614)
     events_C...as Day+1   -0.1499   0.1151      0.136                  (-0.3573, 0.)
     events_C...as Day+2    0.7339   0.3842     <2e-16       ***          (0., 1.013)
     events_E...Ireland]  -0.01943  0.08061      0.560              (-0.1936, 0.1369)
     events_E...eland]-1  -0.06712  0.06116      0.222              (-0.185, 0.03347)
     events_E...eland]-2  -0.01041  0.04662      0.536             (-0.1137, 0.08389)
     events_E...eland]+1  -0.07063  0.07558      0.260             (-0.2234, 0.05592)
     events_E...eland]+2    0.0292  0.05669      0.436             (-0.07435, 0.1489)
      events_Good Friday      0.11  0.09041      0.184             (-0.07418, 0.2658)
    events_Good Friday-1   -0.1989   0.1376      0.090         .        (-0.4209, 0.)
    events_Good Friday-2  -0.04079   0.0806      0.404              (-0.2014, 0.1405)
    events_Good Friday+1  -0.01041  0.04662      0.536             (-0.1137, 0.08389)
    events_Good Friday+2  -0.06712  0.06116      0.222              (-0.185, 0.03347)
     events_I...ence Day    0.1638   0.1163      0.128             (-0.04876, 0.3521)
     events_I...ce Day-1   -0.2614    0.145      0.046         *        (-0.5102, 0.)
     events_I...ce Day-2  -0.01706  0.08653      0.860              (-0.2076, 0.1427)
     events_I...ce Day+1   -0.2123   0.1062      0.018         *          (-0.37, 0.)
     events_I...ce Day+2   -0.1669   0.1038      0.072         . (-0.3499, 0.0005492)
        events_Labor Day  -0.07808   0.1617      0.726              (-0.4006, 0.1922)
      events_Labor Day-1  0.006859   0.1281      0.812              (-0.2418, 0.2553)
      events_Labor Day-2    0.1052  0.09045      0.180              (-0.0614, 0.2776)
      events_Labor Day+1   0.02279  0.08641      0.842              (-0.1538, 0.2086)
      events_Labor Day+2   0.05392   0.2232      0.728               (-0.3901, 0.419)
     events_Memorial Day   -0.5431   0.3007      0.008        **        (-0.8368, 0.)
     events_M...al Day-1   -0.2853   0.1786      0.058         .        (-0.5505, 0.)
     events_M...al Day-2   0.02219  0.07818      0.494              (-0.1831, 0.1708)
     events_M...al Day+1  -0.09167  0.09077      0.226             (-0.2686, 0.03845)
     events_M...al Day+2   0.02977  0.06713      0.432              (-0.1184, 0.1602)
    events_New Years Day   -0.3642   0.2184      0.034         *        (-0.6201, 0.)
     events_N...rs Day-1  -0.05085  0.09856      0.780              (-0.2851, 0.1068)
     events_N...rs Day-2   -0.1679   0.1331      0.146             (-0.4263, 0.01854)
     events_N...rs Day+1    0.3689   0.1968      0.010         *         (0., 0.5615)
     events_N...rs Day+2  -0.01825   0.1102      0.576              (-0.2542, 0.2282)
            events_Other    0.1719   0.1011      0.084         .  (-0.008841, 0.3811)
          events_Other-1  -0.05274   0.0608      0.398             (-0.1599, 0.06659)
          events_Other-2   0.02876  0.05116      0.622             (-0.05771, 0.1344)
          events_Other+1  0.003735  0.06167      0.940              (-0.1134, 0.1245)
          events_Other+2  -0.01036  0.06064      0.860               (-0.1193, 0.122)
     events_Thanksgiving   -0.2246   0.1383      0.058         .        (-0.4136, 0.)
     events_T...giving-1  -0.07548  0.08498      0.256              (-0.2514, 0.0596)
     events_T...giving-2   -0.1614   0.1109      0.098         .        (-0.3446, 0.)
     events_T...giving+1   0.06853  0.09371      0.352              (-0.1606, 0.2407)
     events_T...giving+2  -0.08959  0.09333      0.234             (-0.2859, 0.07873)
     events_Veterans Day   -0.2094    0.146      0.094         .        (-0.4598, 0.)
     events_V...ns Day-1   -0.2939   0.1679      0.024         *        (-0.4777, 0.)
     events_V...ns Day-2   0.09865   0.1308      0.368              (-0.1505, 0.3564)
     events_V...ns Day+1    0.0236  0.07285      0.468               (-0.142, 0.1876)
     events_V...ns Day+2    -0.305   0.1799      0.034         *        (-0.5211, 0.)
           str_dow_2-Tue   0.03979    0.039      0.298              (-0.03265, 0.119)
           str_dow_3-Wed   0.06363  0.04074      0.114             (-0.01497, 0.1326)
           str_dow_4-Thu   0.06377  0.04684      0.172             (-0.02714, 0.1635)
           str_dow_5-Fri  -0.01306  0.03321      0.700             (-0.07789, 0.0549)
           str_dow_6-Sat   -0.1248  0.03981     <2e-16       ***  (-0.2006, -0.05074)
           str_dow_7-Sun  -0.07184  0.05074      0.162              (-0.1631, 0.0296)
                     ct1    0.3804   0.1371      0.008        **     (0.1497, 0.6546)
          is_weekend:ct1   0.05211  0.08246      0.504              (-0.1342, 0.1958)
       str_dow_2-Tue:ct1   -0.2006   0.1743      0.248              (-0.5789, 0.1156)
       str_dow_3-Wed:ct1  -0.07108  0.08401      0.390              (-0.2353, 0.1098)
       str_dow_4-Thu:ct1   -0.1686   0.1231      0.166             (-0.4403, 0.03422)
       str_dow_5-Fri:ct1   0.06852   0.1098      0.532              (-0.1525, 0.2771)
       str_dow_6-Sat:ct1   -0.1557  0.09424      0.088         .   (-0.3201, 0.04374)
       str_dow_7-Sun:ct1    0.2074   0.1487      0.158              (-0.1043, 0.4628)
     ct1:sin1_tow_weekly   -0.1741  0.06427      0.010         *   (-0.2936, -0.0425)
     ct1:cos1_tow_weekly    0.4462   0.2213      0.040         *    (0.01655, 0.8754)
     ct1:sin2_tow_weekly   -0.1284  0.07668      0.076         .   (-0.2611, 0.01516)
     ct1:cos2_tow_weekly    0.4423    0.192      0.018         *    (0.06458, 0.8273)
         sin1_tow_weekly    0.1561  0.04424     <2e-16       ***    (0.06052, 0.2306)
         cos1_tow_weekly -0.005014  0.08071      0.950               (-0.1548, 0.147)
         sin2_tow_weekly  -0.01693   0.0469      0.710             (-0.1123, 0.07638)
         cos2_tow_weekly   0.07175   0.0724      0.340             (-0.05967, 0.2114)
         sin3_tow_weekly  -0.01231   0.0259      0.656             (-0.0591, 0.04309)
         cos3_tow_weekly   0.01155  0.04362      0.796            (-0.07682, 0.08743)
         sin4_tow_weekly   0.01231   0.0259      0.656             (-0.04309, 0.0591)
         cos4_tow_weekly   0.01155  0.04362      0.796            (-0.07682, 0.08743)
      sin1_toq_quarterly    0.0112  0.05998      0.844              (-0.1131, 0.1244)
      cos1_toq_quarterly  -0.09109  0.06665      0.166             (-0.2333, 0.04973)
      sin2_toq_quarterly  0.007969  0.06269      0.876               (-0.104, 0.1444)
      cos2_toq_quarterly  0.009331  0.05823      0.864              (-0.1123, 0.1297)
      sin3_toq_quarterly  -0.08117  0.04946      0.090         .   (-0.187, 0.007812)
      cos3_toq_quarterly -0.007711  0.06378      0.920              (-0.1241, 0.1103)
      sin4_toq_quarterly   -0.0479  0.05649      0.400             (-0.1541, 0.06167)
      cos4_toq_quarterly  -0.06148  0.05895      0.292             (-0.1777, 0.04919)
      sin5_toq_quarterly  -0.07068  0.07321      0.320             (-0.2142, 0.07914)
      cos5_toq_quarterly   0.03178  0.05084      0.508              (-0.06357, 0.132)
                  y_lag1     3.005   0.4797     <2e-16       ***        (1.92, 3.703)
                  y_lag2   -0.3595   0.4018      0.390               (-1.002, 0.4975)
                  y_lag3    0.1069   0.2811      0.712              (-0.4322, 0.6543)
        y_avglag_7_14_21     0.738   0.2791      0.008        **      (0.2137, 1.341)
         y_avglag_1_to_7    0.2454   0.3148      0.424              (-0.3902, 0.8911)
        y_avglag_8_to_14    0.1943   0.2057      0.330              (-0.2124, 0.5986)
    Signif. Code: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    Multiple R-squared: 0.7859,   Adjusted R-squared: 0.7358
    F-statistic: 14.855 on 69 and 296 DF,   p-value: 1.110e-16
    Model AIC: 1504.1,   model BIC: 1779.0

    WARNING: the condition number is large, 9.03e+03. This might indicate that there are strong multicollinearity or other numerical problems.
    WARNING: the F-ratio and its p-value on regularized methods might be misleading, they are provided only for reference purposes.





.. GENERATED FROM PYTHON SOURCE LINES 150-151

We can access the component plots in a similar way.

.. GENERATED FROM PYTHON SOURCE LINES 151-156

.. code-block:: default
   :lineno-start: 152


    # Gets the fig list
    figs = one_by_one_estimator.plot_components()
    # Shows the component plot for 1st model only
    plotly.io.show(figs[0])



.. raw:: html
    :file: /Users/ysu1/Repos/github/greykite/docs/gallery/templates/images/sphx_glr_0400_forecast_one_by_one_001.html






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  57.906 seconds)


.. _sphx_glr_download_gallery_templates_0400_forecast_one_by_one.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 0400_forecast_one_by_one.py <0400_forecast_one_by_one.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 0400_forecast_one_by_one.ipynb <0400_forecast_one_by_one.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
